# Model types: ollama, gemini, scaledown, scaledown-llm
# Target to run DataSet Against
TARGET_MODEL_TYPE=ollama
# Helper Model for things like APO Requests
HELPER_MODEL_TYPE=ollama
# Model that judges answers and scores them
JUDGE_MODEL_TYPE=ollama

# Dataset Configuration
# Set to a file path like "data/simple_qa_test_set.csv", "data/TruthfulQA.csv" or "internal" to use the default dataset.
DATASET_PATH=data/simple_qa_test_set.csv
#ex: 1-50 or leave BLANK to run all
DATASET_RANGE=

# ScaleDown configuration
# Only needed if using model type "scaledown" or "scaledown-llm"
SCALEDOWN_API_KEY=your_scaledown_api_key

# ScaleDown Compression Wrapper (model type "scaledown")
# Compresses prompts before sending to base models
# Compression Rates (0.0 = no compression, 1.0 = maximum compression)
SD_RATE_TARGET=0.7
SD_RATE_HELPER=0.6
SD_RATE_JUDGE=0.0
# For scaledown model type, specify what base model to compress
TARGET_BASE_MODEL=gemini
HELPER_BASE_MODEL=gemini
JUDGE_BASE_MODEL=gemini

##LLM API's

# ScaleDown LLM Wrapper (model type "scaledown-llm") 
# Direct LLM prompting through ScaleDown API
# Model options: gpt-4o, gpt-4, gpt-3.5-turbo, claude-3-opus, etc.
SD_LLM_MODEL_TARGET=gpt-4o
SD_LLM_MODEL_HELPER=gpt-4o
SD_LLM_MODEL_JUDGE=gpt-4o
# Compression rates for ScaleDown LLM
SD_LLM_RATE_TARGET=0.7
SD_LLM_RATE_HELPER=0.6
SD_LLM_RATE_JUDGE=0.0

# Ollama configuration (Local LLM)
# Only needed if using model type "ollama" or scaledown with ollama base
OLLAMA_MODEL=gemma3:27b
OLLAMA_ENDPOINT=http://localhost:11434/api/generate

# Gemini configuration (if using gemini or scaledown model type)
# Only needed if using model type "gemini" or scaledown with gemini base
GEMINI_API_KEY=your_gemini_api_key
GEMINI_MODEL=gemini-1.5-flash

# Debug configuration
PIPE_DEBUG=1
PIPE_DEBUG_MAXLEN=220
PIPE_DEBUG_HTTP=0
# Set to 1 to print all context keys and values at each stage
PIPE_DEBUG_CONTEXT=0

PIPE_STAGE_CONSOLE=summary
PIPE_STAGE_CONSOLE=full
PIPE_STAGE_CONSOLE=none